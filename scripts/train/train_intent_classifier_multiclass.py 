import os
import json
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import RobertaTokenizer, RobertaForSequenceClassification, AdamW, get_scheduler
from tqdm import tqdm

#settings
DATA_PATH = "/data/s3905993/ECRHMAS/data/redial/intent_train.jsonl"
LABELS_PATH = "/data/s3905993/ECRHMAS/data/redial/intent_label_names.json"
MODEL_SAVE_PATH = "/data/s3905993/ECRHMAS/src/models/roberta_intent_classifier"

BATCH_SIZE = 16
EPOCHS = 3
LR = 2e-5
MAX_LEN = 128

#defining intent label set
INTENT_LABELS = ["seeking_recommendation", "feedback", "chit_chat", "other"]

#loading labels
with open(LABELS_PATH, "r") as f:
    INTENT_LABELS = json.load(f)

label2idx = {label: i for i, label in enumerate(INTENT_LABELS)}
idx2label = {i: label for label, i in label2idx.items()}

#dataset
class IntentDataset(Dataset):
    def __init__(self, json_path, tokenizer):
        self.samples = []
        with open(json_path, "r") as f:
            for line in f:
                data = json.loads(line)
                text = data.get("text", "").strip()
                label = int(data.get("label", -1))
                if text and label != -1:
                    self.samples.append((text, label))
        self.tokenizer = tokenizer

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        text, label = self.samples[idx]
        encoded = self.tokenizer(
            text, padding="max_length", truncation=True, max_length=MAX_LEN, return_tensors="pt"
        )
        return {
            "input_ids": encoded["input_ids"].squeeze(0),
            "attention_mask": encoded["attention_mask"].squeeze(0),
            "labels": torch.tensor(label, dtype=torch.long)
        }

def train():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    tokenizer = RobertaTokenizer.from_pretrained("roberta-base")
    model = RobertaForSequenceClassification.from_pretrained("roberta-base", num_labels=len(INTENT_LABELS)).to(device)

    dataset = IntentDataset(DATA_PATH, tokenizer)
    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)

    optimizer = AdamW(model.parameters(), lr=LR)
    scheduler = get_scheduler(
        "linear",
        optimizer=optimizer,
        num_warmup_steps=0,
        num_training_steps=len(dataloader) * EPOCHS
    )

    model.train()
    for epoch in range(EPOCHS):
        total_loss = 0
        progress = tqdm(dataloader, desc=f"Epoch {epoch+1}")
        for batch in progress:
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            scheduler.step()
            optimizer.zero_grad()

            total_loss += loss.item()
            progress.set_postfix(loss=loss.item())
        print(f"Epoch {epoch+1} Loss: {total_loss/len(dataloader):.4f}")

    os.makedirs(MODEL_SAVE_PATH, exist_ok=True)
    model.save_pretrained(MODEL_SAVE_PATH)
    tokenizer.save_pretrained(MODEL_SAVE_PATH)

    #save
    with open(os.path.join(MODEL_SAVE_PATH, "intent_labels.json"), "w") as f:
        json.dump(INTENT_LABELS, f)
    print(f"Model and label mapping saved to: {MODEL_SAVE_PATH}")

if __name__ == "__main__":
    train()
